{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZAjA4RqoZzx",
    "outputId": "21b35957-1234-4e02-9de6-08babfdc3ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "import time\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "tf.random.set_seed(11)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeejPH1Nowx0"
   },
   "source": [
    "## Distillation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "sHp224_PokGu"
   },
   "outputs": [],
   "source": [
    "# Offline Standard Distillation\n",
    "class Distiller(tf.keras.Model):\n",
    "  def __init__(self, teacher, student):\n",
    "      super(Distiller, self).__init__()\n",
    "      \n",
    "      # Attributs de la classe Distiller\n",
    "      self.teacher = teacher\n",
    "      self.student = student\n",
    "      \n",
    "  # Compilation du model\n",
    "  def compile( self, optimizer, metrics, distillation_loss_fn, student_loss_fn, alpha = 0.1, temperature= 20):\n",
    "\n",
    "    super(Distiller,self).compile(optimizer = optimizer, metrics= metrics )\n",
    "    # losses\n",
    "    self.distillation_loss_fn = distillation_loss_fn\n",
    "    self.student_loss_fn = student_loss_fn\n",
    "\n",
    "    # Hyperparameters\n",
    "    self.temperature = temperature\n",
    "    self.alpha = alpha\n",
    "  \n",
    "  # Training Step\n",
    "  def train_step(self, data):\n",
    "    # Unpack data\n",
    "    x, y = data\n",
    "\n",
    "    # Forward pass of teacher\n",
    "    teacher_predictions = self.teacher(x, training=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "      # student forward\n",
    "      student_predictions = self.student(x, training= True)\n",
    "\n",
    "      # Compute losses\n",
    "      student_loss = self.student_loss_fn(y, tf.nn.softmax(student_predictions))\n",
    "      distillation_loss = self.distillation_loss_fn(\n",
    "          tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "          tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "        )\n",
    "\n",
    "      loss = self.alpha * student_loss + (1- self.alpha)* distillation_loss\n",
    "\n",
    "    # Compute gradients\n",
    "    trainable_vars = self.student.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "    # Update weights\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "    # Update the metrics configured in `compile()`.\n",
    "    self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "    # Return a dict of performance\n",
    "    results = {m.name: m.result() for m in self.metrics}\n",
    "    results.update(\n",
    "        {\"student_loss\": student_loss,\"Dist_loss\": distillation_loss, \"loss\": loss}\n",
    "    )\n",
    "    return results\n",
    "\n",
    "  # Test Step\n",
    "  def test_step(self, data):\n",
    "    \n",
    "    # Unpack the data\n",
    "    x, y = data\n",
    "\n",
    "    # Compute predictions\n",
    "    y_prediction = self.student(x, training=False)\n",
    "\n",
    "    # Calculate the loss\n",
    "    student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "    # Update the metrics.\n",
    "    self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "    # Return a dict of performance\n",
    "    results = {m.name: m.result() for m in self.metrics}\n",
    "    results.update({\"student_loss\": student_loss})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgKfpiKNt2Mc"
   },
   "source": [
    "## Quantisized Distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "UrrNBNurt5yj"
   },
   "outputs": [],
   "source": [
    "class Quantized_Distiller(tf.keras.Model):\n",
    "  def __init__(self, teacher, student, nb_bits = 8):\n",
    "      super(Quantized_Distiller, self).__init__()\n",
    "      \n",
    "      # Attributs de la classe Distiller\n",
    "      self.teacher = teacher\n",
    "      self.student = student\n",
    "      self.nb_bits = nb_bits\n",
    "\n",
    "      # Poids du student model\n",
    "      self.original_w = student.get_weights()\n",
    "      self.nb_layers =  len(self.original_w )// 2\n",
    "\n",
    "      self.quantized_w = []\n",
    "\n",
    "     \n",
    " \n",
    "\n",
    "  def matrix2vect(self):\n",
    "    # Vectorisation de la matrice des poids\n",
    "    self.layer_shapes = []\n",
    "    vect = []\n",
    "\n",
    "    for i in range (self.nb_layers):\n",
    "      # rassembler les poids et biais dans une seule matrices\n",
    "      v = np.concatenate( (self.original_w[2*i], np.reshape(self.original_w[2*i+1], (1,-1)) ), axis=0)\n",
    "\n",
    "      # enregistrer les dimensiosn des matrices pour effectuer l'opération inverse\n",
    "      self.layer_shapes.append(v.shape)\n",
    "\n",
    "      # Vectoriser les matrices de poids-biais\n",
    "      vect.append(np.ndarray.flatten(v))\n",
    "    \n",
    "    return np.array(vect)\n",
    "                                               \n",
    "    \n",
    "  def vect2matrix(self, v):\n",
    "    w = []\n",
    "    # Cette fonction permet de transformer les poids quantifiées sous le formats correspondants aux dimensions des poids des couches\n",
    "    for i in range(len(v)):\n",
    "      mat = np.reshape(v[i], self.layer_shapes[i])\n",
    "      w.append(mat[0:-1])\n",
    "      w.append(mat[-1])\n",
    "    return np.array(w)\n",
    "\n",
    "  # Transforme les valeurs d'un vecteur vers l'intervalle [0, 1]\n",
    "  def scale_function_b(self,tab, bucket_size):\n",
    "    Vecteur = []\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    for tab in tab:\n",
    "      if bucket_size > len(tab):\n",
    "        raise ValueError(f'Bucket_size ({bucket_size}) must be smaller than or equal to the vector length ({len(tab)})')\n",
    "      \n",
    "      v = np.zeros((len(tab)))\n",
    "      nb_bucket = len(tab)//bucket_size\n",
    "      \n",
    "      # Nombre de bucket pair\n",
    "      if len(tab) % bucket_size == 0:\n",
    "        nb_param = nb_bucket\n",
    "        alpha = np.ones((nb_param))\n",
    "        beta = np.zeros((nb_param))\n",
    "\n",
    "        for i in range(nb_param):\n",
    "          beta[i] = np.min(tab[i*bucket_size: (i+1)*bucket_size])\n",
    "          alpha[i] = np.max(tab[i*bucket_size: (i+1)*bucket_size]) - np.min(tab[i*bucket_size: (i+1)*bucket_size])\n",
    "\n",
    "          if alpha[i] == 0:\n",
    "            alpha[i] = 1\n",
    "          v[i*bucket_size: (i+1)*bucket_size] = (tab[i*bucket_size: (i+1)*bucket_size] - beta[i]) /(alpha[i])\n",
    "      \n",
    "      # Nombre de bucket impair\n",
    "      else:\n",
    "        nb_param = nb_bucket + 1\n",
    "        alpha = np.zeros((nb_param))\n",
    "        beta = np.zeros((nb_param))\n",
    "\n",
    "        for i in range(nb_param):\n",
    "          if i == nb_param - 1:\n",
    "            beta[i] = np.min(tab[i*bucket_size : -1])\n",
    "            alpha[i] = np.max(tab[i*bucket_size : -1]) - np.min(tab[i*bucket_size : -1])\n",
    "\n",
    "            if alpha[i] == 0:\n",
    "              alpha[i] = 1\n",
    "\n",
    "            v[ i*bucket_size : -1] = (tab[i*bucket_size : -1] - beta[i]) /(alpha[i])\n",
    "          else:\n",
    "            beta[i] = np.min(tab[i*bucket_size: (i+1)*bucket_size])\n",
    "            alpha[i] = np.max(tab[i*bucket_size: (i+1)*bucket_size]) - np.min(tab[i*bucket_size: (i+1)*bucket_size])\n",
    "\n",
    "            if alpha[i] == 0:\n",
    "              alpha[i] = 1  \n",
    "\n",
    "            v[i*bucket_size: (i+1)*bucket_size] = (tab[i*bucket_size: (i+1)*bucket_size] - beta[i]) /(alpha[i])\n",
    "\n",
    "      A.append(alpha)\n",
    "      B.append(beta)\n",
    "      Vecteur.append(v)  \n",
    "\n",
    "    # Renvoie le vecteur mis à l'échelle [0, 1] et les coéfficients de remise à l'échelle alpha, beta\n",
    "    return np.array(Vecteur), np.array(A), np.array(B)\n",
    "\n",
    "  def invert_scale_function_b(self,v, alpha, beta, bucket_size):\n",
    "\n",
    "    unscalled_v = []\n",
    "    layer = 0\n",
    "    for v in v:\n",
    "      if bucket_size > len(v):\n",
    "        raise ValueError(f'Bucket_size ({bucket_size}) must be smaller than or equal to the vector length ({len(v)})')\n",
    "\n",
    "      Q = np.zeros((len(v[layer])))\n",
    "      nb_param = len(alpha[layer])\n",
    "\n",
    "      if len(v) % (nb_param * bucket_size) == 0:\n",
    "        for i in range(nb_param):\n",
    "          Q[i*bucket_size: (i+1)*bucket_size] = alpha[layer][i] * v[i*bucket_size: (i+1)*bucket_size] + beta[layer][i]\n",
    "\n",
    "      else:\n",
    "        for i in range(nb_param):\n",
    "          if i == nb_param - 1:\n",
    "            Q[i*bucket_size: -1] = alpha[i] * v[i*bucket_size: -1] + beta[i]\n",
    "          else:\n",
    "            Q[i*bucket_size: (i+1)*bucket_size] = alpha[layer][i] * v[i*bucket_size: (i+1)*bucket_size] + beta[layer][i]\n",
    "      \n",
    "      unscalled_v.append(Q)\n",
    "      return unscalled_v\n",
    "\n",
    "\n",
    "  def uniform_quantification( self, Vect):\n",
    "    v_q = []\n",
    "    s = 2**self.nb_bits\n",
    "    for v in Vect:\n",
    "      k = s*v - np.floor(v*s)\n",
    "      eps = np.zeros(len(k))\n",
    "      eps[k>0.5] = 1\n",
    "      Q = np.floor(v*s)/s + (eps/s)\n",
    "\n",
    "      v_q.append(Q)\n",
    "    return np.array(v_q)\n",
    "  \n",
    "\n",
    "\n",
    "  # Compilation du model\n",
    "  def compile( self, optimizer, metrics, distillation_loss_fn, student_loss_fn, alpha = 0.1, temperature= 20, bucket_size = 32):\n",
    "\n",
    "    super(Quantized_Distiller,self).compile(optimizer = optimizer, metrics= metrics )\n",
    "    # losses\n",
    "    self.distillation_loss_fn = distillation_loss_fn\n",
    "    self.student_loss_fn = student_loss_fn\n",
    "\n",
    "    # Hyperparameters\n",
    "    self.temperature = temperature\n",
    "    self.alpha = alpha\n",
    "    self.bucket_size = bucket_size\n",
    "  \n",
    "\n",
    "  # Training Step\n",
    "  def train_step(self, data):\n",
    "    # Unpack data\n",
    "    x, y = data\n",
    "\n",
    "    # Forward pass of teacher\n",
    "    teacher_predictions = self.teacher(x, training=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "      ## QUantification du student\n",
    "      # Vectorisation des poids\n",
    "      v = self.matrix2vect()\n",
    "      \n",
    "      # scalling \n",
    "      scalled_v, self.q_alpha, self.q_beta = self.scale_function_b(v, self.bucket_size)\n",
    "\n",
    "      # Quantification Uniforme\n",
    "      v_q = self.uniform_quantification(scalled_v)\n",
    "      \n",
    "      #Transformer le vecteur de poids auntifiées en matrices de poids\n",
    "      self.quantized_w = self.vect2matrix(v_q)\n",
    "      \n",
    "\n",
    "      # Appliquer les poids aux student model\n",
    "      self.student.set_weights(self.quantized_w)\n",
    "\n",
    "      ## student forward\n",
    "      student_predictions = self.student(x, training= True)\n",
    "\n",
    "      # Compute losses\n",
    "      student_loss = self.student_loss_fn(y, tf.nn.softmax(student_predictions))\n",
    "      distillation_loss = self.distillation_loss_fn(\n",
    "          tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "          tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "        )\n",
    "\n",
    "      loss = self.alpha * student_loss + (1- self.alpha)* distillation_loss\n",
    "\n",
    "    # Compute gradients\n",
    "    trainable_vars = self.student.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "    # Update weights\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "    # Update the metrics configured in `compile()`.\n",
    "    self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "    # Mise à jour des poids\n",
    "    \n",
    "\n",
    "    self.original_w = []\n",
    "    for t in self.student.trainable_variables:\n",
    "      self.original_w.append(t[:])\n",
    "      print(t[:][0])\n",
    "\n",
    "\n",
    "    print(\"Chargement des poids effectués\")\n",
    "    # Mise à jour des poids quantifiés\n",
    "    # Vectorisation des poids\n",
    "    v = self.matrix2vect()\n",
    "\n",
    "    # scalling \n",
    "    scalled_v, self.q_alpha, self.q_beta = self.scale_function_b(v, self.bucket_size)\n",
    "\n",
    "    # Quantification Uniforme\n",
    "    v_q = self.uniform_quantification(scalled_v)\n",
    "\n",
    "    #Transformer le vecteur de poids auntifiées en matrices de poids\n",
    "    self.quantized_w = self.vect2matrix(v_q)\n",
    "\n",
    "    # Return a dict of performance\n",
    "    results = {m.name: m.result() for m in self.metrics}\n",
    "    results.update(\n",
    "        {\"loss\": loss, \"student_loss\": student_loss,\"Dist_loss\": distillation_loss }\n",
    "    )\n",
    "    return results\n",
    "\n",
    "  # Test Step\n",
    "  def test_step(self, data):\n",
    "    \n",
    "    # Unpack the data\n",
    "    x, y = data\n",
    "\n",
    "    # Compute predictions\n",
    "    y_prediction = self.student(x, training=False)\n",
    "\n",
    "    # Calculate the loss\n",
    "    student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "    # Update the metrics.\n",
    "    self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "    # Return a dict of performance\n",
    "    results = {m.name: m.result() for m in self.metrics}\n",
    "    results.update({\"student_loss\": student_loss})\n",
    "    return results\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mnRH_1FotB-"
   },
   "source": [
    "## Ploting history fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "gHg6sicTooTT"
   },
   "outputs": [],
   "source": [
    "def plot_hist(los1,los2, accur1, accur2):\n",
    "  plt.figure(figsize= (20,7))\n",
    "  plt.subplot(121)\n",
    "  plt.plot(accur1, label='KD Accuracy')\n",
    "  plt.plot(accur2, label= 'Scratch Accuracy')\n",
    "\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.grid()\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  plt.subplot(122)\n",
    "  plt.plot(los1, label='KD Loss')\n",
    "  plt.plot(los2,  label= 'Scratch Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "\n",
    "  plt.grid()\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww7QWLC_ozrQ"
   },
   "source": [
    "## Loading and processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "wmarStinjrgC"
   },
   "outputs": [],
   "source": [
    "def pad_dataset(x, y, pad=2):\n",
    "  x_train = np.zeros((x.shape[0],28+ 2*pad, 28+ 2*pad))\n",
    "  x_test = np.zeros((y.shape[0], 28+ 2*pad, 28+ 2*pad))\n",
    "  for i in range(len(x)):\n",
    "    x_train[i] = np.pad(x[i], pad)\n",
    "    if i < y.shape[0]:\n",
    "      x_test[i]= np.pad(y[i], pad)\n",
    "\n",
    "  return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qPAdXvQo5Lv",
    "outputId": "8c0b77c4-9f55-4c61-862e-96f82893e2ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "x_test shape: (10000, 28, 28)\n",
      "x_train shape: (60000, 32, 32)\n",
      "x_test shape: (10000, 32, 32)\n",
      "x_train shape: (60000, 32, 32, 1)\n",
      "x_test shape: (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "\n",
    "\n",
    "x_train, x_test = pad_dataset(x_train, x_test)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train, (-1,32,32,1))\n",
    "x_test = np.reshape(x_test, (-1,32,32,1))\n",
    "\n",
    "\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WXAb7DAbjO8X",
    "outputId": "55bc0cb0-0138-4c38-87d1-7ff91b50bd96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0762e578d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1000,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMdRh1W0o6qw"
   },
   "source": [
    "## Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "HmKcXZe6MeXa"
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = layers.Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = layers.Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = layers.Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "DQGnWRZdMfvJ"
   },
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature \n",
    "    map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of \n",
    "    filters is\n",
    "    doubled. Within each stage, the layers have the same number \n",
    "    filters and the same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tf.keras.layers.add([x, y])\n",
    "            x = layers.Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = layers.AveragePooling2D(pool_size=8)(x)\n",
    "    y = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(num_classes,\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_H77Qpo2o8zR",
    "outputId": "5e5602e5-65ff-4ed0-9670-38b40e625caa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 13ms/step - loss: 0.1197 - sparse_categorical_accuracy: 0.9740\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 15)   150         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 15)  60          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 15)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 15)   2040        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 15)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 15)   2040        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 15)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 15)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 15)   2040        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 15)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 15)   2040        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 15)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 15)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 15)   2040        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 15)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 15)   2040        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 15)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 15)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 15)   2040        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 15)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 15)   2040        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 15)  60          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 15)   0           ['activation_6[0][0]',           \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 15)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 30)   4080        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 30)  120         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 30)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 30)   8130        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 30)   480         ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 30)  120         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 30)   0           ['conv2d_11[0][0]',              \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 30)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 30)   8130        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 30)  120         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 30)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 30)   8130        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 30)  120         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 30)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 30)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 30)   8130        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 30)  120         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 30)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 30)   8130        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 30)  120         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 16, 16, 30)   0           ['activation_12[0][0]',          \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 30)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 30)   8130        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 30)  120         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 30)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 30)   8130        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 30)  120         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 30)   0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 30)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 60)     16260       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 60)    240         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 60)     0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 60)     32460       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 60)     1860        ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 60)    240         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 60)     0           ['conv2d_20[0][0]',              \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 60)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 8, 8, 60)     32460       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 60)    240         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 8, 8, 60)     0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 8, 8, 60)     32460       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 8, 8, 60)    240         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 8, 8, 60)     0           ['activation_18[0][0]',          \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 8, 8, 60)     0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 8, 8, 60)     32460       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8, 8, 60)    240         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 8, 8, 60)     0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 8, 8, 60)     32460       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 8, 8, 60)    240         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 8, 8, 60)     0           ['activation_20[0][0]',          \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 8, 60)     0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 8, 8, 60)     32460       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 60)    240         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 8, 8, 60)     0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 8, 8, 60)     32460       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 8, 8, 60)    240         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 8, 8, 60)     0           ['activation_22[0][0]',          \n",
      "                                                                  'batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 8, 8, 60)     0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 60)    0           ['activation_24[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 60)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           610         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 327,310\n",
      "Trainable params: 325,600\n",
      "Non-trainable params: 1,710\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the student\n",
    "teacher =tf.keras.models.load_model(\"/content/drive/MyDrive/Stage/saved_models_KD/Resnet26_KD_mnist\")\n",
    "teacher.evaluate(x_test, y_test)\n",
    "teacher.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iajl8Z90r-QL"
   },
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IS8PaSE7YUH",
    "outputId": "c0ad5305-adbf-4b5d-f165-7eb2ad918ca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mémoire occupée par le student model:  0.134216  Mo\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 32)                32800     \n",
      "                                                                 \n",
      " re_lu_30 (ReLU)             (None, 32)                0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " re_lu_31 (ReLU)             (None, 16)                0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " re_lu_32 (ReLU)             (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,554\n",
      "Trainable params: 33,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def stud_model(): \n",
    "  student = tf.keras.Sequential()\n",
    "\n",
    "  input = tf.keras.Input((32, 32, 1))\n",
    "\n",
    "  x = tf.keras.layers.Flatten()(input)\n",
    "  x = tf.keras.layers.Dense(32)(x)\n",
    "  x = tf.keras.layers.ReLU()(x)\n",
    "  x = tf.keras.layers.Dense(16)(x)\n",
    "  x = tf.keras.layers.ReLU()(x)\n",
    "  x = tf.keras.layers.Dense(8)(x)\n",
    "  x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "  output = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "  student = tf.keras.Model(input, output)\n",
    "  return student\n",
    "\n",
    "student = stud_model()\n",
    "\n",
    "student_scratch = tf.keras.models.clone_model(student)\n",
    "print(\"Mémoire occupée par le student model: \",(student_scratch.count_params()*4) /1e6,' Mo')\n",
    "student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ap-C63Ywf6oq",
    "outputId": "ca0d79b6-1ca5-4cda-cd81-5b39a516bd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 32)\n",
      "(32,)\n",
      "(32, 16)\n",
      "(16,)\n",
      "(16, 8)\n",
      "(8,)\n",
      "(8, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "w = student.get_weights()\n",
    "student.set_weights(w)\n",
    "for i in  range(len(student.get_weights())):\n",
    "  print(student.get_weights()[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z4q2ldaySO7"
   },
   "source": [
    "# Knowledge Distillation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlyvxRMnybj9"
   },
   "source": [
    "## Entrainement par KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A0uEmORryRHB",
    "outputId": "58477578-6b4a-42f2-f1c3-c357d2d8b42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_2:0\", shape=(32,), dtype=float32)\n",
      "Tensor(\"strided_slice_5:0\", shape=(), dtype=float32)\n",
      "Tensor(\"strided_slice_8:0\", shape=(16,), dtype=float32)\n",
      "Tensor(\"strided_slice_11:0\", shape=(), dtype=float32)\n",
      "Tensor(\"strided_slice_14:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"strided_slice_17:0\", shape=(), dtype=float32)\n",
      "Tensor(\"strided_slice_20:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"strided_slice_23:0\", shape=(), dtype=float32)\n",
      "Chargement des poids effectués\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-1c95daf048c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Distill teacher to student\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdist_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtime_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m       func_outputs = nest.map_structure(\n\u001b[1;32m   1146\u001b[0m           convert, func_outputs, expand_composites=True)\n\u001b[0;32m-> 1147\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m       check_func_mutation(func_args_before, func_kwargs_before, func_args,\n\u001b[1;32m   1149\u001b[0m                           func_kwargs, original_func)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        `Model.compile` is called. You can skip the cache and generate again the\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        \n    File \"<ipython-input-43-2439e0db86ce>\", line 217, in train_step\n        v = self.matrix2vect()\n    File \"<ipython-input-43-2439e0db86ce>\", line 26, in matrix2vect\n        v = np.concatenate( (self.original_w[2*i], np.reshape(self.original_w[2*i+1], (1,-1)) ), axis=0)\n    File \"<__array_function__ internals>\", line 6, in reshape\n        \n    File \"/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\", line 298, in reshape\n        return _wrapfunc(a, 'reshape', newshape, order=order)\n    File \"/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\", line 54, in _wrapfunc\n        return _wrapit(obj, method, *args, **kwds)\n    File \"/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\", line 43, in _wrapit\n        result = getattr(asarray(obj), method)(*args, **kwds)\n\n    NotImplementedError: Cannot convert a symbolic Tensor (strided_slice_3:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
     ]
    }
   ],
   "source": [
    "# Paramètre d'entrainement\n",
    "Epochs = 100\n",
    "Batch = 32\n",
    "\n",
    "# Construction du distilleur\n",
    "student = stud_model()\n",
    "dist = Quantized_Distiller(teacher, student)\n",
    "dist.compile(\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate= 0.1, momentum=0.9),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    distillation_loss_fn = tf.keras.losses.KLDivergence(),\n",
    "    student_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    alpha = 0.2,\n",
    "    temperature = 5,\n",
    "    bucket_size = 10)\n",
    "\n",
    "\n",
    "# Distill teacher to student\n",
    "t1 = time.time()\n",
    "dist_hist = dist.fit(x_train, y_train, epochs=Epochs, batch_size = Batch)\n",
    "t2 = time.time()\n",
    "time_dist = t2 - t1\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "dist.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTBz0DhzsIDX"
   },
   "source": [
    "## Evaluating both performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DerNoEHAON0"
   },
   "outputs": [],
   "source": [
    "def plot_hist2(teacher, student_KD, student_scratch):\n",
    "\n",
    "  # Definir les variables\n",
    "  accur1 = student_KD.history['sparse_categorical_accuracy'] \n",
    "  accur2 = student_scratch.history['sparse_categorical_accuracy'] \n",
    "  accur3 = teacher.history['sparse_categorical_accuracy'] \n",
    "\n",
    "  los1 = student_KD.history['student_loss']\n",
    "  los2 = student_scratch.history['loss']\n",
    "  los3 = teacher.history['loss']\n",
    "\n",
    "  # Plot\n",
    "  plt.figure(figsize= (20,7))\n",
    "  plt.subplot(121)\n",
    "  plt.plot(accur1, label='KD Accuracy')\n",
    "  plt.plot(accur2, label= 'Scratch Accuracy')\n",
    "  plt.plot(accur3, label= 'Teacher Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.grid()\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  plt.subplot(122)\n",
    "  plt.plot(los1, label='KD Loss')\n",
    "  plt.plot(los2,  label= 'Scratch Loss')\n",
    "  plt.plot(los3, label= 'Teacher Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.grid()\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hQ3N7fasLNJ"
   },
   "outputs": [],
   "source": [
    "#plot_hist2(Teacher_hist, dist_hist, scrratch_stud_hist)\n",
    "plot_hist(dist_hist.history['student_loss'], scrratch_stud_hist.history['loss'] ,dist_hist.history['sparse_categorical_accuracy'] , scrratch_stud_hist.history['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-14K0FX2zfjw"
   },
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WEANjXT5oln"
   },
   "outputs": [],
   "source": [
    "print(\"Training Time :\")\n",
    "print(\" Knowledge Distillation :\", time_dist // 60,'Min', time_dist - (time_dist // 60)*60,\"s\")\n",
    "print(\" Training from Scratch :\", time_scratch // 60 ,\"Min\", time_scratch - (time_scratch // 60) * 60, \"s\")\n",
    "print(\"\")\n",
    "\n",
    "Compression_ratio = teacher.count_params()/ student.count_params()\n",
    "print(\"Estimation des paramètres mémoire :\")\n",
    "print(\" Ratio de compression :\", Compression_ratio )\n",
    "print(\" Compression effectuée : \", (teacher.count_params()*4)/1e6, \"Mo à \", (student.count_params()*4) /1e6 ,\"Mo\"  )\n",
    "print(\"\")\n",
    "\n",
    "print(\"Accuracy on test set: \")\n",
    "print(\" Teacher accuracy \", teacher.evaluate(x_test, y_test)[1])\n",
    "print(\" KD Student accuracy \", dist.evaluate(x_test, y_test)[0])\n",
    "print(\" Scratch Student accuracy \", student_scratch.evaluate(x_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukeH6gi5lan5"
   },
   "outputs": [],
   "source": [
    "student.save(\"/content/drive/MyDrive/Stage/saved_models_KD/DNN_KD_mnist_33000param\")\n",
    "student.save_weights(\"/content/drive/MyDrive/Stage/saved_models_KD/dnn_mnist_33kparam.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-5zIDbK5-b3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FeejPH1Nowx0",
    "0mnRH_1FotB-",
    "ww7QWLC_ozrQ",
    "GMdRh1W0o6qw",
    "iajl8Z90r-QL",
    "BTBz0DhzsIDX"
   ],
   "name": "Resnet26_KD_DNN_Quantif_dist",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
