{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZAjA4RqoZzx",
    "outputId": "21b35957-1234-4e02-9de6-08babfdc3ef0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "tf.random.set_seed(11)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgKfpiKNt2Mc"
   },
   "source": [
    "## Quantized Distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantized_Distiller(tf.keras.Model):\n",
    "  def __init__(self, teacher, student, nb_bits = 8):\n",
    "      super(Quantized_Distiller, self).__init__()\n",
    "      \n",
    "      # Attributs de la classe Distiller\n",
    "      self.teacher = teacher\n",
    "      self.student = student\n",
    "      self.nb_bits = nb_bits\n",
    "\n",
    "      # Poids du student model\n",
    "      self.original_w = self.student.get_weights()\n",
    "      self.nb_layers =  len(self.original_w )// 2\n",
    "\n",
    "      self.quantized_w = []\n",
    "\n",
    "     \n",
    " \n",
    "  def  matrix2vect(self,w):\n",
    "    # Vectorisation de la matrice des poids\n",
    "    self.layer_shapes = []\n",
    "    vect = []\n",
    "\n",
    "    for i in range (4):\n",
    "      # rassembler les poids et biais dans une seule matrices\n",
    "      v = tf.concat(  [w[2*i], tf.reshape(w[2*i+1], (1,-1)) ], axis=0)\n",
    "\n",
    "      # enregistrer les dimensiosn des matrices pour effectuer l'opération inverse\n",
    "      self.layer_shapes.append(v.shape)\n",
    "\n",
    "      # Vectoriser les matrices de poids-biais\n",
    "      vect.append(tf.reshape(v, (-1, 1)))\n",
    "    \n",
    "    return vect\n",
    "                                               \n",
    "  def vect2matrix(self, v):\n",
    "      w = []\n",
    "      # Cette fonction permet de transformer les poids quantifiées sous le formats correspondants aux dimensions des poids des couches\n",
    "      for i in range(len(v)):\n",
    "        mat = tf.reshape(v[i], self.layer_shapes[i])\n",
    "        w.append(mat[0:-1])\n",
    "        w.append(mat[-1])\n",
    "      return w\n",
    "\n",
    "\n",
    "  # Transforme les valeurs d'un vecteur vers l'intervalle [0, 1]\n",
    "  def scale_function(self, tab, bucket_size):\n",
    "      \n",
    "      Vecteur = []\n",
    "      A = []\n",
    "      B = []\n",
    "\n",
    "      for tab in tab:\n",
    "        \n",
    "        if bucket_size > tab.shape[0]:\n",
    "          raise ValueError(f'Bucket_size ({bucket_size}) must be smaller than or equal to the vector length ({len(tab)})')\n",
    "        v = tf.constant([])\n",
    "        alpha = []\n",
    "        beta = []\n",
    "\n",
    "        nb_bucket = tab.shape[0]//bucket_size\n",
    "\n",
    "        # Nombre de bucket pair\n",
    "        if tab.shape[0] % bucket_size == 0:\n",
    "          nb_param = nb_bucket\n",
    "\n",
    "\n",
    "          for i in range(nb_param):\n",
    "            b = tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "            a = tf.math.reduce_max(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "            alpha.append(a)\n",
    "            beta.append(b)\n",
    "\n",
    "            \"\"\" if a[0] == 0:\n",
    "                  a = tf.constant([1])\"\"\"\n",
    "            vect = (tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) - b) / a\n",
    "            v = tf.concat([v, vect], 0)\n",
    "\n",
    "        # Nombre de bucket impair\n",
    "        else:\n",
    "          nb_param = nb_bucket + 1\n",
    "\n",
    "          for i in range(nb_param):\n",
    "            if i == nb_param - 1:\n",
    "              b = tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) )\n",
    "              a = tf.math.reduce_max(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) )\n",
    "              alpha.append(a)\n",
    "              beta.append(b)\n",
    "\n",
    "              \"\"\" if a[0] == 0:\n",
    "                  a = tf.constant([1])\"\"\"\n",
    "\n",
    "              vect = (  tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]))  - b) /(a)\n",
    "\n",
    "            else:\n",
    "              b = tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "              a = tf.math.reduce_max(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "              alpha.append(a)\n",
    "              beta.append(b)\n",
    "\n",
    "              \"\"\" if a[0] == 0:\n",
    "                  a = tf.constant([1])\"\"\"\n",
    "\n",
    "              vect = (tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) - b) / a\n",
    "            v = tf.concat([v, vect], 0)\n",
    "\n",
    "        A.append(alpha)\n",
    "        B.append(beta)\n",
    "        Vecteur.append(v) \n",
    "    \n",
    "      return Vecteur, A, B\n",
    "\n",
    "  def uniform_quantification(self, Vect):\n",
    "      v_q = []\n",
    "      s = tf.constant([2**self.nb_bits], dtype=tf.float32)\n",
    "\n",
    "      for v in Vect:\n",
    "        k = tf.math.subtract(tf.math.multiply(v,s) , tf.math.floor( tf.math.multiply(v,s)) )\n",
    "        eps = tf.ones(k.shape[0])\n",
    "        \n",
    "        eps = tf.where( tf.greater(k,0.5),eps, 0 )\n",
    "        v_s = tf.math.multiply(v,s)\n",
    "        res =  tf.math.floor(tf.math.divide(v_s, s))\n",
    "\n",
    "        Q =  res + (tf.math.divide(eps ,s) )\n",
    "\n",
    "        v_q.append(Q)\n",
    "      return v_q\n",
    "\n",
    "  def invert_scale_function_b(self,v, alpha, beta, bucket_size):\n",
    "\n",
    "    unscalled_v = []\n",
    "    layer = 0\n",
    "    for v in v:\n",
    "      if bucket_size > len(v):\n",
    "        raise ValueError(f'Bucket_size ({bucket_size}) must be smaller than or equal to the vector length ({len(v)})')\n",
    "\n",
    "      Q = np.zeros((len(v[layer])))\n",
    "      nb_param = len(alpha[layer])\n",
    "\n",
    "      if len(v) % (nb_param * bucket_size) == 0:\n",
    "        for i in range(nb_param):\n",
    "          Q[i*bucket_size: (i+1)*bucket_size] = alpha[layer][i] * v[i*bucket_size: (i+1)*bucket_size] + beta[layer][i]\n",
    "\n",
    "      else:\n",
    "        for i in range(nb_param):\n",
    "          if i == nb_param - 1:\n",
    "            Q[i*bucket_size: -1] = alpha[i] * v[i*bucket_size: -1] + beta[i]\n",
    "          else:\n",
    "            Q[i*bucket_size: (i+1)*bucket_size] = alpha[layer][i] * v[i*bucket_size: (i+1)*bucket_size] + beta[layer][i]\n",
    "      \n",
    "      unscalled_v.append(Q)\n",
    "      return unscalled_v\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  # Compilation du model\n",
    "  def compile( self, optimizer, metrics, distillation_loss_fn, student_loss_fn, alpha = 0.1, temperature= 20, bucket_size = 32):\n",
    "\n",
    "    super(Quantized_Distiller,self).compile(optimizer = optimizer, metrics= metrics )\n",
    "    # losses\n",
    "    self.distillation_loss_fn = distillation_loss_fn\n",
    "    self.student_loss_fn = student_loss_fn\n",
    "\n",
    "    # Hyperparameters\n",
    "    self.temperature = temperature\n",
    "    self.alpha = alpha\n",
    "    self.bucket_size = bucket_size\n",
    "  \n",
    "\n",
    "  # Training Step\n",
    "  def train_step(self, data):\n",
    "    self.original_w = self.student.weights \n",
    "    \n",
    "    # Unpack data\n",
    "    x, y = data\n",
    "\n",
    "    # Forward pass of teacher\n",
    "    teacher_predictions = self.teacher(x, training=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "      ## QUantification du student\n",
    "      # Vectorisation des poids\n",
    "      v = self.matrix2vect(self.original_w)\n",
    "      \n",
    "      # scalling \n",
    "      scalled_v, self.q_alpha, self.q_beta = self.scale_function(v, self.bucket_size)\n",
    "\n",
    "      # Quantification Uniforme\n",
    "      v_q = self.uniform_quantification(scalled_v)\n",
    "      \n",
    "      #Transformer le vecteur de poids auntifiées en matrices de poids\n",
    "      self.quantized_w = self.vect2matrix(v_q)\n",
    "      \n",
    "\n",
    "      \n",
    "\n",
    "      ## student forward\n",
    "      student_predictions = self.student(x, training= True)\n",
    "\n",
    "      # Compute losses\n",
    "      student_loss = self.student_loss_fn(y, tf.nn.softmax(student_predictions))\n",
    "      distillation_loss = self.distillation_loss_fn(\n",
    "          tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "          tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "        )\n",
    "\n",
    "      loss = self.alpha * student_loss + (1- self.alpha)* distillation_loss\n",
    "\n",
    "    # Compute gradients\n",
    "    trainable_vars = self.student.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "    # Update weights\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "    # Update the metrics configured in `compile()`.\n",
    "    self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "    # Mise à jour des poids quantifiés\n",
    "    self.original_w = self.student.weights\n",
    "    # Vectorisation des poids\n",
    "    v = self.matrix2vect(self.original_w)\n",
    "\n",
    "    # scalling \n",
    "    scalled_v, self.q_alpha, self.q_beta = self.scale_function(v, self.bucket_size)\n",
    "\n",
    "    # Quantification Uniforme\n",
    "    v_q = self.uniform_quantification(scalled_v)\n",
    "\n",
    "    #Transformer le vecteur de poids auntifiées en matrices de poids\n",
    "    self.quantized_w = self.vect2matrix(v_q)\n",
    "\n",
    "    # Return a dict of performance\n",
    "    results = {m.name: m.result() for m in self.metrics}\n",
    "    results.update(\n",
    "        {\"loss\": loss, \"student_loss\": student_loss,\"Dist_loss\": distillation_loss }\n",
    "    )\n",
    "    return results\n",
    "\n",
    "  # Test Step\n",
    "  def test_step(self, data):\n",
    "    \n",
    "    # Unpack the data\n",
    "    x, y = data\n",
    "\n",
    "    # Compute predictions\n",
    "    y_prediction = self.student(x, training=False)\n",
    "\n",
    "    # Calculate the loss\n",
    "    student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "    # Update the metrics.\n",
    "    self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "    # Return a dict of performance\n",
    "    results = {m.name: m.result() for m in self.metrics}\n",
    "    results.update({\"student_loss\": student_loss})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mnRH_1FotB-"
   },
   "source": [
    "## Ploting history fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "gHg6sicTooTT"
   },
   "outputs": [],
   "source": [
    "def plot_hist(los1,los2, accur1, accur2):\n",
    "  plt.figure(figsize= (20,7))\n",
    "  plt.subplot(121)\n",
    "  plt.plot(accur1, label='KD Accuracy')\n",
    "  plt.plot(accur2, label= 'Scratch Accuracy')\n",
    "\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.grid()\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  plt.subplot(122)\n",
    "  plt.plot(los1, label='KD Loss')\n",
    "  plt.plot(los2,  label= 'Scratch Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "\n",
    "  plt.grid()\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww7QWLC_ozrQ"
   },
   "source": [
    "## Loading and processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qPAdXvQo5Lv",
    "outputId": "8c0b77c4-9f55-4c61-862e-96f82893e2ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Data Loading ================\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "y_test shape: (10000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================ Data Loading ================\")\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMdRh1W0o6qw"
   },
   "source": [
    "## Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "HmKcXZe6MeXa"
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = layers.Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = layers.Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = layers.Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "DQGnWRZdMfvJ"
   },
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature \n",
    "    map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of \n",
    "    filters is\n",
    "    doubled. Within each stage, the layers have the same number \n",
    "    filters and the same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tf.keras.layers.add([x, y])\n",
    "            x = layers.Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = layers.AveragePooling2D(pool_size=8)(x)\n",
    "    y = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(num_classes,\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_H77Qpo2o8zR",
    "outputId": "5e5602e5-65ff-4ed0-9670-38b40e625caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Loading teacher model ================\n",
      "Evaluation of Teacher model!\n",
      "313/313 [==============================] - 12s 36ms/step - loss: 0.5328 - sparse_categorical_accuracy: 0.9054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"================ Loading teacher model ================\")\n",
    "teacher = tf.keras.models.load_model(\"Resnet26_from_logits\")\n",
    "print(\"Evaluation of Teacher model!\")\n",
    "teacher.evaluate(x_test, y_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iajl8Z90r-QL"
   },
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IS8PaSE7YUH",
    "outputId": "c0ad5305-adbf-4b5d-f165-7eb2ad918ca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mémoire occupée par le student model:  0.39636  Mo\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 32)                98336     \n",
      "                                                                 \n",
      " re_lu_39 (ReLU)             (None, 32)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " re_lu_40 (ReLU)             (None, 16)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " re_lu_41 (ReLU)             (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 10)                90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 99,090\n",
      "Trainable params: 99,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def stud_model(): \n",
    "  student = tf.keras.Sequential()\n",
    "\n",
    "  input = tf.keras.Input((32, 32, 3))\n",
    "\n",
    "  x = tf.keras.layers.Flatten()(input)\n",
    "  x = tf.keras.layers.Dense(32)(x)\n",
    "  x = tf.keras.layers.ReLU()(x)\n",
    "  x = tf.keras.layers.Dense(16)(x)\n",
    "  x = tf.keras.layers.ReLU()(x)\n",
    "  x = tf.keras.layers.Dense(8)(x)\n",
    "  x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "  output = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "  student = tf.keras.Model(input, output)\n",
    "  return student\n",
    "\n",
    "student = stud_model()\n",
    "\n",
    "student_scratch = tf.keras.models.clone_model(student)\n",
    "print(\"Mémoire occupée par le student model: \",(student_scratch.count_params()*4) /1e6,' Mo')\n",
    "student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ap-C63Ywf6oq",
    "outputId": "ca0d79b6-1ca5-4cda-cd81-5b39a516bd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 32)\n",
      "(32,)\n",
      "(32, 16)\n",
      "(16,)\n",
      "(16, 8)\n",
      "(8,)\n",
      "(8, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "w = student.get_weights()\n",
    "student.set_weights(w)\n",
    "for i in  range(len(student.get_weights())):\n",
    "  print(student.get_weights()[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z4q2ldaySO7"
   },
   "source": [
    "# Knowledge Distillation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlyvxRMnybj9"
   },
   "source": [
    "## Entrainement par KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A0uEmORryRHB",
    "outputId": "58477578-6b4a-42f2-f1c3-c357d2d8b42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# Paramètre d'entrainement\n",
    "Epochs = 100\n",
    "Batch = 32\n",
    "\n",
    "# Construction du distilleur\n",
    "student = stud_model()\n",
    "\n",
    "student.compile(\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate= 0.1, momentum=0.9),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    )\n",
    "\n",
    "\n",
    "# Distill teacher to student\n",
    "t1 = time.time()\n",
    "hist= student.fit(x_train, y_train, epochs=Epochs, batch_size = Batch)\n",
    "t2 = time.time()\n",
    "time_dist = t2 - t1\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "student.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FeejPH1Nowx0",
    "0mnRH_1FotB-",
    "ww7QWLC_ozrQ",
    "GMdRh1W0o6qw",
    "iajl8Z90r-QL",
    "BTBz0DhzsIDX"
   ],
   "name": "Resnet26_KD_DNN_Quantif_dist",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
