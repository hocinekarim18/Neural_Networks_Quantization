{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f259a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot;\n",
    "import tensorflow as tf;\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e10b8d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Data Loading ================\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================ Data Loading ================\")\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "x_train = np.reshape(x_train, (-1,28, 28, 1))\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb9e47",
   "metadata": {},
   "source": [
    "## Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "380c7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=2, padding=\"valid\"),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=2, padding=\"valid\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=120, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=84, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddc41c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karimhocine/.local/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2977 - sparse_categorical_accuracy: 0.9108 - val_loss: 0.1064 - val_sparse_categorical_accuracy: 0.9674\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0964 - sparse_categorical_accuracy: 0.9711 - val_loss: 0.0796 - val_sparse_categorical_accuracy: 0.9760\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.0483 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0511 - sparse_categorical_accuracy: 0.9842 - val_loss: 0.0465 - val_sparse_categorical_accuracy: 0.9865\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0428 - sparse_categorical_accuracy: 0.9868 - val_loss: 0.0481 - val_sparse_categorical_accuracy: 0.9853\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0363 - sparse_categorical_accuracy: 0.9888 - val_loss: 0.0374 - val_sparse_categorical_accuracy: 0.9864\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0349 - val_sparse_categorical_accuracy: 0.9882\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9918 - val_loss: 0.0311 - val_sparse_categorical_accuracy: 0.9909\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0345 - val_sparse_categorical_accuracy: 0.9897\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.0350 - val_sparse_categorical_accuracy: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f297859ec70>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data= (x_test, y_test),\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d2342",
   "metadata": {},
   "source": [
    "## Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0d50fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:18\u001b[0;36m\u001b[0m\n\u001b[0;31m    def vect2matrix(self, v):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def  matrix2vect(self,w):\n",
    "    # Vectorisation de la matrice des poids\n",
    "    self.layer_shapes = []\n",
    "    vect = []\n",
    "\n",
    "    for i in range (4):\n",
    "      # rassembler les poids et biais dans une seule matrices\n",
    "      v = tf.concat(  [w[2*i], tf.reshape(w[2*i+1], (1,-1)) ], axis=0)\n",
    "\n",
    "      # enregistrer les dimensiosn des matrices pour effectuer l'opération inverse\n",
    "      self.layer_shapes.append(v.shape)\n",
    "\n",
    "      # Vectoriser les matrices de poids-biais\n",
    "      vect.append(tf.reshape(v, (-1, 1)))\n",
    "    \n",
    "    return vect\n",
    "                                               \n",
    "def vect2matrix(self, v):\n",
    "    w = []\n",
    "    # Cette fonction permet de transformer les poids quantifiées sous le formats correspondants aux dimensions des poids des couches\n",
    "    for i in range(len(v)):\n",
    "        mat = tf.reshape(v[i], self.layer_shapes[i])\n",
    "        w.append(mat[0:-1])\n",
    "        w.append(mat[-1])\n",
    "    return w\n",
    "\n",
    "\n",
    "  # Transforme les valeurs d'un vecteur vers l'intervalle [0, 1]\n",
    "def scale_function(self, tab, bucket_size):\n",
    "\n",
    "  Vecteur = []\n",
    "  A = []\n",
    "  B = []\n",
    "\n",
    "  for tab in tab:\n",
    "\n",
    "    if bucket_size > tab.shape[0]:\n",
    "      raise ValueError(f'Bucket_size ({bucket_size}) must be smaller than or equal to the vector length ({len(tab)})')\n",
    "    v = tf.constant([])\n",
    "    alpha = []\n",
    "    beta = []\n",
    "\n",
    "    nb_bucket = tab.shape[0]//bucket_size\n",
    "\n",
    "    # Nombre de bucket pair\n",
    "    if tab.shape[0] % bucket_size == 0:\n",
    "      nb_param = nb_bucket\n",
    "\n",
    "\n",
    "      for i in range(nb_param):\n",
    "        b = tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "        a = tf.math.reduce_max(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "        alpha.append(a)\n",
    "        beta.append(b)\n",
    "\n",
    "        \"\"\" if a[0] == 0:\n",
    "              a = tf.constant([1])\"\"\"\n",
    "        vect = (tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) - b) / a\n",
    "        v = tf.concat([v, vect], 0)\n",
    "\n",
    "    # Nombre de bucket impair\n",
    "    else:\n",
    "      nb_param = nb_bucket + 1\n",
    "\n",
    "      for i in range(nb_param):\n",
    "        if i == nb_param - 1:\n",
    "          b = tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) )\n",
    "          a = tf.math.reduce_max(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) )\n",
    "          alpha.append(a)\n",
    "          beta.append(b)\n",
    "\n",
    "          \"\"\" if a[0] == 0:\n",
    "              a = tf.constant([1])\"\"\"\n",
    "\n",
    "          vect = (  tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]))  - b) /(a)\n",
    "\n",
    "        else:\n",
    "          b = tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "          a = tf.math.reduce_max(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "          alpha.append(a)\n",
    "          beta.append(b)\n",
    "\n",
    "          \"\"\" if a[0] == 0:\n",
    "              a = tf.constant([1])\"\"\"\n",
    "\n",
    "          vect = (tf.slice(tab[:][0], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) - b) / a\n",
    "        v = tf.concat([v, vect], 0)\n",
    "\n",
    "    A.append(alpha)\n",
    "    B.append(beta)\n",
    "    Vecteur.append(v) \n",
    "\n",
    "    return Vecteur, A, B\n",
    "\n",
    "def uniform_quantification(self, Vect):\n",
    "    v_q = []\n",
    "    s = tf.constant([2**self.nb_bits], dtype=tf.float32)\n",
    "\n",
    "    for v in Vect:\n",
    "    k = tf.math.subtract(tf.math.multiply(v,s) , tf.math.floor( tf.math.multiply(v,s)) )\n",
    "    eps = tf.ones(k.shape[0])\n",
    "\n",
    "    eps = tf.where( tf.greater(k,0.5),eps, 0 )\n",
    "    v_s = tf.math.multiply(v,s)\n",
    "    res =  tf.math.floor(tf.math.divide(v_s, s))\n",
    "\n",
    "    Q =  res + (tf.math.divide(eps ,s) )\n",
    "\n",
    "    v_q.append(Q)\n",
    "    return v_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49846ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
